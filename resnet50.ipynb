{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'train_cluster_cleaned'\n",
    "\n",
    "cluster_numbers = list(range(1, 29))\n",
    "cluster_folders = [os.path.join(data_dir, f'cluster_{i}') for i in cluster_numbers]\n",
    "\n",
    "# Data preprocessing loop\n",
    "train_images, val_images = [], []\n",
    "train_labels, val_labels = [], []\n",
    "\n",
    "for folder in cluster_folders:\n",
    "    cluster_images = []\n",
    "    cluster_labels = []\n",
    "    \n",
    "    file_names = os.listdir(folder)\n",
    "    for file_name in file_names:\n",
    "        cell_id = int(file_name.split('_')[1].split('.')[0])\n",
    "        img = cv2.imread(os.path.join(folder, file_name))\n",
    "        \n",
    "        cluster_images.append(img)\n",
    "        cluster_labels.append(f'cluster_{folder.split(\"_\")[-1]}')\n",
    "    \n",
    "    # Perform train-validation split for each cluster\n",
    "    cluster_train_images, cluster_val_images, cluster_train_labels, cluster_val_labels = train_test_split(\n",
    "        cluster_images, cluster_labels, test_size=0.2, random_state=42\n",
    "    )  # 80% train, 20% validation\n",
    "\n",
    "    # Merge train-validation sets of each cluster\n",
    "    train_images.extend(cluster_train_images)\n",
    "    val_images.extend(cluster_val_images)\n",
    "    train_labels.extend(cluster_train_labels)\n",
    "    val_labels.extend(cluster_val_labels)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "lb = sklearn.preprocessing.LabelEncoder()\n",
    "train_labels = lb.fit_transform(train_labels)\n",
    "val_labels = lb.transform(val_labels)\n",
    "\n",
    "val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=len(cluster_numbers))\n",
    "val_images = val_images.astype('float32') / 255.0\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=len(cluster_numbers))\n",
    "train_images = train_images.astype('float32') / 255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "num_classes = len(cluster_numbers)\n",
    "\n",
    "# Define the custom object ClassToken\n",
    "class ClassToken(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ClassToken, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.class_token = self.add_weight(\n",
    "            shape=(1, 1, input_shape[3]),\n",
    "            initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "            name=\"class_token\",\n",
    "        )\n",
    "        super(ClassToken, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # Add a class token\n",
    "        class_token = tf.broadcast_to(self.class_token, [batch_size, 1, 1, tf.shape(self.class_token)[2]])\n",
    "        x = tf.concat([class_token, x], axis=1)\n",
    "        return x\n",
    "\n",
    "# Create ImageDataGenerators for training and validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate train and validation data from the ImageDataGenerators\n",
    "train_generator = train_datagen.flow(\n",
    "    x=train_images,\n",
    "    y=train_labels,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow(\n",
    "    x=val_images,\n",
    "    y=val_labels,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model without the top (classification) layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Fine-tune only the top layers, freeze the rest\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a new classification head to the base model\n",
    "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "predictions = layers.Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "# Save the fine-tuned model\n",
    "model.save(\"resnet_50_cells_100_epoch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output_csv folder if it doesn't exist\n",
    "output_folder = \"output_Image4_csv\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Save the training data to a CSV file\n",
    "training_data = pd.DataFrame({\n",
    "    'epoch': np.arange(1, len(history.history['accuracy']) + 1),\n",
    "    'train_accuracy': history.history['accuracy'],\n",
    "    'validation_accuracy': history.history['val_accuracy'],\n",
    "    'loss': history.history['loss'],\n",
    "    'validation_loss': history.history['val_loss']\n",
    "})\n",
    "training_data.to_csv(os.path.join(output_folder, 'history_cleaned_cluster_cnn.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RUN TO HERE<h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset directory\n",
    "dataset_dir = 'data/data_processed/cell_images_cleaned'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing image to matrix array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "    return img_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = os.listdir(dataset_dir)\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_directory = os.path.join(dataset_dir, class_name)\n",
    "    for image_name in os.listdir(class_directory):\n",
    "        images.append(os.path.join(class_directory, image_name))\n",
    "        labels.append(label)\n",
    "\n",
    "# Train validation split\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    images, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    directory=None,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(32, 32),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    directory=None,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(32, 32),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained ResNet50 model, without the top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model without the top (classification) layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Fine-tune only the top layers, freeze the rest\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a new classification head to the base model\n",
    "x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "predictions = layers.Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=100\n",
    ")\n",
    "# Save the fine-tuned model\n",
    "model.save(\"resnet_50_cells_100_epoch.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# for index, row in val_df.iterrows():\n",
    "#     image_path = row['filename']\n",
    "#     true_label = row['class']\n",
    "#     image_id = os.path.basename(image_path).split('.')[0]  # Assuming the ID is the filename without the extension\n",
    "\n",
    "#     img_array = preprocess_image(image_path, target_size=(32, 32))\n",
    "#     predictions = model.predict(img_array)\n",
    "#     predicted_label = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "#     is_correct = true_label == predicted_label\n",
    "#     results.append([image_id, is_correct])\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Image_ID', 'Prediction_Correct'])\n",
    "results_df.to_csv('validation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/trainHistoryDict', 'wb') as file_pi:\n",
    "    pickle.dump(model_history.history, file_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
